From Wikipedia:

"In [mathematical optimization](https://en.wikipedia.org/wiki/Mathematical_optimization "Mathematical optimization"), a **trust region** is the subset of the region of the [objective function](https://en.wikipedia.org/wiki/Objective_function "Objective function") that is approximated using a model function (often a [quadratic](https://en.wikipedia.org/wiki/Quadratic_function "Quadratic function")). If an adequate model of the objective function is found within the trust region, then the region is expanded; conversely, if the approximation is poor, then the region is contracted.

Trust-region methods are in some sense dual to [line-search](https://en.wikipedia.org/wiki/Line-search "Line-search") methods: trust-region methods first choose a step size (the size of the trust region) and then a step direction, while line-search methods first choose a step direction and then a step size.""


### Trust Region Algorithms
The algorithm described here is a simplification of the one used in my thesis (1). So, for more information, see the [thesis](http://www.applied-mathematics.net/mythesis/Thesis.html)

### KKT condition
- [Karush–Kuhn–Tucker conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)
- [Karush-Kuhn-Tucker (KKT) Conditions](http://apmonitor.com/me575/index.php/Main/KuhnTucker): Has solved exercises and short videos with explanations.

## Courses and videos
- [EIN 6935 Nonlinear Optimization and Game Theory](https://www.chkwon.net/teaching/ein-6935/)
- [Numerical Optimization](https://web.stanford.edu/class/cme304/): CME 304/MS&E 315 - Winter Quarter 2015-2016  Professor Walter Murray
- [A Crash Course on Optimization](https://bids.berkeley.edu/events/crash-course-optimization)
- [Nonlinear Programming: 3rd Edition](http://www.athenasc.com/nonlinbook.html)
- [Optimization Techniques in Engineering](https://flow.byu.edu/me575)
- [JORGE NOCEDAL | Optimization methods for TRAINING DEEP NEURAL NETWORKS](https://www.youtube.com/watch?v=qE3cPuyzU_8) based on the paper.

## Books
- [Bertsekas's books](http://web.mit.edu/dimitrib/www/Convex_Alg_Chapters.html): "This series of complementary textbooks cover all aspects of continuous optimization, and its connections with discrete optimization via duality. The two convex optimization books deal primarily with convex, possibly nondifferentiable, problems and rely on convex analysis. By contrast the nonlinear programming book focuses primarily on analytical and computational methods for possibly nonconvex differentiable problems. It relies primarily on calculus and variational analysis, yet it still contains a detailed presentation of duality theory and its uses for both convex and nonconvex problems."
## Articles
- [An Interactive Tutorial on Numerical Optimization](https://www.benfrederickson.com/numerical-optimization/)
- [What are the some best books on Non Linear optimization?](https://www.quora.com/What-are-the-some-best-books-on-Non-Linear-optimization)
- [Optimization Methods for Large-Scale Machine Learning](https://arxiv.org/pdf/1606.04838.pdf)

## Lectures
- [KKT](https://www.youtube.com/watch?v=HIm3Z0L90Co):  Lecture 40(A): Kuhn-Tucker Conditions: Conceptual and geometric insight
- [Duality: Lagrangian and dual problem](https://www.youtube.com/watch?v=4OifjG2kIJQ)

## Software
- [GEKKO Optimization Suite](https://gekko.readthedocs.io/en/latest/#gekko-optimization-suite)
